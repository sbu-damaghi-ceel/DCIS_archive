{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load the HE cell assignment from assignHE.py\n",
    "\"\"\"\n",
    "# import pandas as pd\n",
    "# cell_path = '/mnt/data10/shared/yujie/DCIS/ANALYSIS/HE_cellAssignment/exc44_assigned_CL_100_5.csv'\n",
    "\n",
    "# assigned_cells_df = pd.read_csv(cell_path)  \n",
    "\n",
    "# assigned_cells_df['clusterId'] = assigned_cells_df['clusterId'].fillna('None')\n",
    "# none_cluster_counts = assigned_cells_df[assigned_cells_df['clusterId'] == 'None'].groupby('patientNum').size()\n",
    "# none_cluster_counts_df = none_cluster_counts.reset_index(name='None_Count')\n",
    "\n",
    "# total_counts = assigned_cells_df.groupby('patientNum').size()\n",
    "# total_counts_df =total_counts.reset_index(name='total_Count')\n",
    "\n",
    "# counts_df = pd.merge(none_cluster_counts_df, total_counts_df, on='patientNum')\n",
    "# counts_df['None_percentage'] = counts_df.apply(\n",
    "#     lambda row: row['None_Count'] / row['total_Count'],\n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# # Display the result\n",
    "# print(counts_df)\n",
    "\n",
    "# #NA means the HE cell does NOT belong to any IHC-community\n",
    "\n",
    "\n",
    "'''\n",
    "stats for number of ducts\n",
    "'''\n",
    "import pandas as pd\n",
    "# cell_path = '/mnt/data10/shared/yujie/DCIS/ANALYSIS/HE_cellAssignment/exc44_assigned_CL_100_5.csv'\n",
    "\n",
    "# assigned_cells_df = pd.read_csv(cell_path)\n",
    "# assigned_cells_df.groupby(['slideId','ductId']).count()\n",
    "\n",
    "cell_path = '/mnt/data10/shared/yujie/DCIS/ANALYSIS/HE_cellAssignment/biopsy_assigned_CL_100_5.csv'\n",
    "\n",
    "assigned_cells_df = pd.read_csv(cell_path)\n",
    "assigned_cells_df.groupby(['slideId','ductId']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import pickle\n",
    "from scipy.stats import kurtosis,skew\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "########################################################################################\n",
    "#load morphology features and texture features for each HE cell then aggregate at ROI level\n",
    "def AggreCellFeatures(text_folder,morph_folder,assigned_cells_df ,out_path):\n",
    "    \n",
    "    cells_df = assigned_cells_df.dropna(axis=0)#drop unassigned cells\n",
    "    print(f'original count:{len(assigned_cells_df)} ,no-NA count(HE cells within neighborhood):{len(cells_df)}')\n",
    "\n",
    "    df_list=[]\n",
    "    for slideId in cells_df['slideId'].unique():\n",
    "        print(f'{slideId} starts processing ...')\n",
    "        if not os.path.exists(os.path.join(morph_folder,f'{slideId}.json')):\n",
    "            print(f'{slideId}.json does not exist')\n",
    "            continue\n",
    "        with open(os.path.join(morph_folder,f'{slideId}.json'),'r') as f:\n",
    "            morph_feat=json.load(f)\n",
    "        with open(os.path.join(text_folder,f'{slideId}.pkl'),'rb') as f:\n",
    "            text_feat=pickle.load(f)\n",
    "        cell_slide = cells_df[cells_df['slideId']==slideId]\n",
    "        morph_df=pd.DataFrame(morph_feat)\n",
    "        text_df = pd.DataFrame(text_feat)\n",
    "        merge1_df = pd.merge(cell_slide, morph_df, left_on='Object ID', right_on='cellId', how='left')\n",
    "        slide_df = pd.merge(merge1_df, text_df, on='cellId', how='left')\n",
    "        df_list.append(slide_df)\n",
    "    concat_df = pd.concat(df_list)\n",
    "    final_df=concat_df.drop(columns=['CurvMean','CurvStd','CurvMax','CurvMin']).dropna(axis=0)\n",
    "\n",
    "    #print(final_df.columns)\n",
    "    excluded_columns = ['Object ID', 'patientNum', 'slideId', 'ductId', 'layer', 'clusterId', 'X', 'Y', 'cellId']\n",
    "    included_columns = final_df.columns.difference(excluded_columns)\n",
    "\n",
    "    def kurtosis_func(x):\n",
    "        return kurtosis(x, fisher=True)\n",
    "\n",
    "    def skewness_func(x):\n",
    "        return skew(x)\n",
    "    agg_dict = {col: ['mean', 'std', 'max', 'min', kurtosis_func, skewness_func] for col in included_columns}\n",
    "    flat_columns = []\n",
    "    for main_col, sub_cols in agg_dict.items():\n",
    "        for op in sub_cols:\n",
    "            if callable(op):  # For custom functions, use the function's name\n",
    "                op_name = op.__name__.replace('_func', '')\n",
    "            else:\n",
    "                op_name = op\n",
    "            flat_columns.append(f'{main_col}_{op_name}')\n",
    "    ####################aggregate at community level\n",
    "    comm_agg = final_df.groupby(['slideId', 'clusterId'], as_index=False).agg(agg_dict)\n",
    "    comm_agg.columns = ['slideId', 'clusterId'] + flat_columns\n",
    "    #print(comm_agg.head())\n",
    "    #remove rows with NAs\n",
    "    comm_agg=comm_agg.dropna()\n",
    "    if out_path is not None:\n",
    "        comm_agg.to_csv(out_path,index=False)\n",
    "    return comm_agg\n",
    "########################################################################################\n",
    "#Concat spatial function values and ihc counts from R (merge column-wise) \n",
    "#(row number of cell_aggre is far less than spat )[BECAUSE SEGMENTATION QUALITY, DUCTS IN HE AER UNDER_SEGMENTED]\n",
    "def concatCellSpat(comm_agg,spat,counts,id_df,out_path):\n",
    "    spat['patientNum'] = spat['patient_cluster'].apply(lambda x: x.split('_')[0])\n",
    "    spat['clusterId'] = spat['patient_cluster'].apply(lambda x: x.split('_')[1])\n",
    "    spat=pd.merge(spat,id_df[['patientNum','slideId']],on='patientNum')\n",
    "    spat = spat.drop(columns=['patientNum','patient_cluster'])\n",
    "\n",
    "    counts=counts.dropna(axis=1,how='all')\n",
    "    counts['patientNum'] = counts['patient_cluster'].apply(lambda x: x.split('_')[0])\n",
    "    counts['clusterId'] = counts['patient_cluster'].apply(lambda x: x.split('_')[1])\n",
    "    counts=pd.merge(counts,id_df[['patientNum','slideId']],on='patientNum')\n",
    "    counts = counts.drop(columns=['patientNum','patient_cluster','count_Glut1'])###EXCLUDE Glut1 \n",
    "\n",
    "    comm_agg['clusterId'] = comm_agg['clusterId'].astype(str)\n",
    "    merge1=pd.merge(comm_agg,spat,on=['slideId','clusterId'])\n",
    "    merged_df = pd.merge(merge1,counts,on=['slideId','clusterId'])\n",
    "\n",
    "    ##Remove columns with any NA(usually its 0 or a lot)\n",
    "    final_df = merged_df.dropna(axis=1,how='any')\n",
    "    print(f'merged_df col no. {len(merged_df.columns)},final_df column no. {len(final_df.columns)}')\n",
    "    final_df.to_csv(out_path,index=False)\n",
    "    \n",
    "    return final_df\n",
    "##################################################\n",
    "#####PREPARE TRAINING/VALIDATION SPLIT\n",
    "def prepareSplit(final_df,id_df,json_path):\n",
    "    # Get the upstage Upstage\n",
    "    final_df = pd.merge(final_df, id_df[['slideId','Upstage']], on = \"slideId\")\n",
    "    #print(final_df['Upstage'].value_counts())\n",
    "\n",
    "    experiment_indexes = {}\n",
    "    positive_slideIds = final_df[final_df['Upstage'] == 1]['slideId'].unique()\n",
    "    for valid_slideId in positive_slideIds:\n",
    "        validation_set_positives = final_df[final_df['slideId'] == valid_slideId]\n",
    "        remaining_data = final_df[~(final_df['slideId'] == valid_slideId)]\n",
    "        experiment_indexes[int(valid_slideId)] = {}\n",
    "        for i in range(3):\n",
    "            np.random.seed(i)\n",
    "\n",
    "            sampled_validation_negatives_indexes = remaining_data[remaining_data['Upstage'] == 0].sample(n=len(validation_set_positives)).index\n",
    "            training_positives_indexes = remaining_data[remaining_data['Upstage'] == 1].index\n",
    "            if len(training_positives_indexes) > len(remaining_data[remaining_data['Upstage'] == 0]):\n",
    "                sampled_training_negatives_indexes = remaining_data[remaining_data['Upstage'] == 0].sample(n=len(training_positives_indexes),\n",
    "                                                                                                          replace=True).index\n",
    "            else:\n",
    "                sampled_training_negatives_indexes = remaining_data[remaining_data['Upstage'] == 0].sample(n=len(training_positives_indexes)).index\n",
    "\n",
    "            experiment_indexes[valid_slideId][i]={\n",
    "                'validation_indexes': validation_set_positives.index.union(sampled_validation_negatives_indexes).tolist(),\n",
    "                'training_indexes': training_positives_indexes.union(sampled_training_negatives_indexes).tolist()\n",
    "            }\n",
    "    \n",
    "    with open(json_path,'w') as f:\n",
    "        json.dump(experiment_indexes,f,indent=4)\n",
    "    return experiment_indexes\n",
    "########################################################################################\n",
    "#Plot the ROI count for each slide(upstage vs indolent)\n",
    "def plotROIdist(final_df,id_df):\n",
    "\n",
    "    final_df['slideId'] = final_df['slideId'].astype(str)\n",
    "    id_df['slideId'] = id_df['slideId'].astype(str)\n",
    "    merged_df = pd.merge(final_df, id_df, on='slideId')\n",
    "\n",
    "    count_per_slideId = merged_df.groupby(['slideId', 'Upstage']).size().reset_index(name='counts')\n",
    "\n",
    "    indolent_df = count_per_slideId[count_per_slideId['Upstage'] == 0]\n",
    "    upstage_df = count_per_slideId[count_per_slideId['Upstage'] > 0]\n",
    "\n",
    "    indolent_df = indolent_df.sort_values(by='counts', ascending=True)\n",
    "    upstage_df = upstage_df.sort_values(by='counts', ascending=True)\n",
    "\n",
    "    # Plotting\n",
    "    positions_a = np.arange(len(indolent_df))\n",
    "    positions_b = np.arange(len(indolent_df), len(indolent_df) + len(upstage_df))\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(positions_a, indolent_df['counts'], label='Indolent')\n",
    "    plt.bar(positions_b, upstage_df['counts'], label='Upstage', color='orange')\n",
    "    plt.axvline(x=max(positions_a), color='r', linestyle='--')\n",
    "    plt.xticks(np.concatenate([positions_a, positions_b]), np.concatenate([indolent_df['slideId'], upstage_df['slideId']]), rotation=45, ha='right')\n",
    "\n",
    "    plt.xlabel('Slide ID')\n",
    "    plt.ylabel('Count of Samples')\n",
    "    plt.title('Sample Counts per Slide ID by Upstage Status')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Total number of samples Indolent: {indolent_df[\"counts\"].sum()}')\n",
    "    print(f'Total number of samples Upstage: {upstage_df[\"counts\"].sum()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WHOLE DUCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CA9&LAMP2b 100 5\n",
    "\n",
    "stage = 'biopsy'\n",
    "id_df = pd.read_csv('/mnt/data10/shared/yujie/DCIS/patientNum_slideId_upstage_biopsy.csv')\n",
    "text_folder = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/{stage}_HE_cell_features/texture'\n",
    "morph_folder = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/{stage}_HE_cell_features/morphology'\n",
    "assignedCell_path = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/HE_cellAssignment/{stage}_assigned_CL_100_5.csv'\n",
    "\n",
    "#from process_spatialFunc.ipynb\n",
    "spat=pd.read_csv(f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/R_analysis/{stage}_CL_100_5/spat_integration.csv')\n",
    "counts = pd.read_csv(f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/R_analysis/{stage}_CL_100_5/ihc_counts.csv')\n",
    "\n",
    "result_dir = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/CL_100_5_result/{stage}'\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "comm_agg_path = os.path.join(result_dir,'commAgg_morph_text.csv')\n",
    "final_df_path = os.path.join(result_dir,'allFeatures.csv')\n",
    "json_path = os.path.join(result_dir,'split_index.json')\n",
    "\n",
    "####################RUN\n",
    "assigned_cells_df = pd.read_csv(assignedCell_path) \n",
    "\n",
    "comm_agg=AggreCellFeatures(text_folder,morph_folder,assigned_cells_df ,comm_agg_path)\n",
    "print('AGGREGATE CELL FEATURES DONE')\n",
    "final_df = concatCellSpat(comm_agg,spat,counts,id_df,final_df_path)\n",
    "print('MERGE CELL FEATURES WITH SPATIAL FEATURES DONE')\n",
    "\n",
    "plotROIdist(final_df,id_df)\n",
    "\n",
    "###########################################################################RUN#########################\n",
    "stage = 'exc44'\n",
    "id_df = pd.read_csv('/mnt/data10/shared/yujie/DCIS/patientNum_slideId_upstage_excision.csv')\n",
    "text_folder = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/{stage}_HE_cell_features/texture'\n",
    "morph_folder = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/{stage}_HE_cell_features/morphology'\n",
    "assignedCell_path = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/HE_cellAssignment/{stage}_assigned_CL_100_5.csv'\n",
    "\n",
    "#from process_spatialFunc.ipynb\n",
    "spat=pd.read_csv(f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/R_analysis/{stage}_CL_100_5/spat_integration.csv')\n",
    "counts = pd.read_csv(f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/R_analysis/{stage}_CL_100_5/ihc_counts.csv')\n",
    "\n",
    "\n",
    "result_dir = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/CL_100_5_result/{stage}'\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "comm_agg_path = os.path.join(result_dir,'commAgg_morph_text.csv')\n",
    "final_df_path = os.path.join(result_dir,'allFeatures.csv')\n",
    "json_path = os.path.join(result_dir,'split_index.json')\n",
    "\n",
    "#########################RUN\n",
    "assigned_cells_df = pd.read_csv(assignedCell_path) \n",
    "\n",
    "comm_agg=AggreCellFeatures(text_folder,morph_folder,assigned_cells_df ,comm_agg_path)\n",
    "print('AGGREGATE CELL FEATURES DONE')\n",
    "final_df = concatCellSpat(comm_agg,spat,counts,id_df,final_df_path)\n",
    "print('MERGE CELL FEATURES WITH SPATIAL FEATURES DONE')\n",
    "experiment_indexes = prepareSplit(final_df,id_df,json_path)\n",
    "print('GENERATE TRAIN/VALID SPLIT DONE')\n",
    "comm_agg = pd.read_csv(comm_agg_path)\n",
    "final_df = pd.read_csv(final_df_path)\n",
    "\n",
    "plotROIdist(final_df,id_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CA9 100 5\n",
    "\n",
    "stage = 'biopsy'\n",
    "id_df = pd.read_csv('/mnt/data10/shared/yujie/DCIS/patientNum_slideId_upstage_biopsy.csv')\n",
    "text_folder = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/{stage}_HE_cell_features/texture'\n",
    "morph_folder = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/{stage}_HE_cell_features/morphology'\n",
    "assignedCell_path = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/HE_cellAssignment/{stage}_assigned_CA9_100_5.csv'\n",
    "\n",
    "#from process_spatialFunc.ipynb\n",
    "spat=pd.read_csv(f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/R_analysis/{stage}_C_100_5/spat_integration.csv')\n",
    "counts = pd.read_csv(f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/R_analysis/{stage}_C_100_5/ihc_counts.csv')\n",
    "\n",
    "result_dir = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/CA9_100_5_result/{stage}'\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "comm_agg_path = os.path.join(result_dir,'commAgg_morph_text.csv')\n",
    "final_df_path = os.path.join(result_dir,'allFeatures.csv')\n",
    "json_path = os.path.join(result_dir,'split_index.json')\n",
    "\n",
    "####################RUN\n",
    "assigned_cells_df = pd.read_csv(assignedCell_path) \n",
    "\n",
    "comm_agg=AggreCellFeatures(text_folder,morph_folder,assigned_cells_df ,comm_agg_path)\n",
    "print('AGGREGATE CELL FEATURES DONE')\n",
    "final_df = concatCellSpat(comm_agg,spat,counts,id_df,final_df_path)\n",
    "print('MERGE CELL FEATURES WITH SPATIAL FEATURES DONE')\n",
    "\n",
    "plotROIdist(final_df,id_df)\n",
    "\n",
    "###########################################################################RUN#########################\n",
    "stage = 'exc44'\n",
    "id_df = pd.read_csv('/mnt/data10/shared/yujie/DCIS/patientNum_slideId_upstage_excision.csv')\n",
    "text_folder = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/{stage}_HE_cell_features/texture'\n",
    "morph_folder = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/{stage}_HE_cell_features/morphology'\n",
    "assignedCell_path = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/HE_cellAssignment/{stage}_assigned_CA9_100_5.csv'\n",
    "\n",
    "#from process_spatialFunc.ipynb\n",
    "spat=pd.read_csv(f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/R_analysis/{stage}_C_100_5/spat_integration.csv')\n",
    "counts = pd.read_csv(f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/R_analysis/{stage}_C_100_5/ihc_counts.csv')\n",
    "\n",
    "\n",
    "result_dir = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/CA9_100_5_result/{stage}'\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "comm_agg_path = os.path.join(result_dir,'commAgg_morph_text.csv')\n",
    "final_df_path = os.path.join(result_dir,'allFeatures.csv')\n",
    "json_path = os.path.join(result_dir,'split_index.json')\n",
    "\n",
    "#########################RUN\n",
    "assigned_cells_df = pd.read_csv(assignedCell_path) \n",
    "\n",
    "comm_agg=AggreCellFeatures(text_folder,morph_folder,assigned_cells_df ,comm_agg_path)\n",
    "print('AGGREGATE CELL FEATURES DONE')\n",
    "final_df = concatCellSpat(comm_agg,spat,counts,id_df,final_df_path)\n",
    "print('MERGE CELL FEATURES WITH SPATIAL FEATURES DONE')\n",
    "experiment_indexes = prepareSplit(final_df,id_df,json_path)\n",
    "print('GENERATE TRAIN/VALID SPLIT DONE')\n",
    "comm_agg = pd.read_csv(comm_agg_path)\n",
    "final_df = pd.read_csv(final_df_path)\n",
    "\n",
    "plotROIdist(final_df,id_df)\n",
    "\n",
    "## handle the difference in feature numbers between exc and biopsy\n",
    "# import pandas as pd\n",
    "# final_exc = pd.read_csv('/mnt/data10/shared/yujie/DCIS/ANALYSIS/CA9_100_5_result/exc44/allFeatures.csv')\n",
    "# final_biopsy = pd.read_csv('/mnt/data10/shared/yujie/DCIS/ANALYSIS/CA9_100_5_result/biopsy/allFeatures.csv')\n",
    "# final_exc_sub = final_exc[final_biopsy.columns]\n",
    "# final_exc_sub.to_csv('/mnt/data10/shared/yujie/DCIS/ANALYSIS/CA9_100_5_result/exc44/allFeatures.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LAMP2b 100 5\n",
    "\n",
    "stage = 'biopsy'\n",
    "id_df = pd.read_csv('/mnt/data10/shared/yujie/DCIS/patientNum_slideId_upstage_biopsy.csv')\n",
    "text_folder = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/{stage}_HE_cell_features/texture'\n",
    "morph_folder = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/{stage}_HE_cell_features/morphology'\n",
    "assignedCell_path = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/HE_cellAssignment/{stage}_assigned_LAMP2b_100_5.csv'\n",
    "\n",
    "#from process_spatialFunc.ipynb\n",
    "spat=pd.read_csv(f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/R_analysis/{stage}_L_100_5/spat_integration.csv')\n",
    "counts = pd.read_csv(f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/R_analysis/{stage}_L_100_5/ihc_counts.csv')\n",
    "\n",
    "result_dir = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/LAMP2b_100_5_result/{stage}'\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "comm_agg_path = os.path.join(result_dir,'commAgg_morph_text.csv')\n",
    "final_df_path = os.path.join(result_dir,'allFeatures.csv')\n",
    "json_path = os.path.join(result_dir,'split_index.json')\n",
    "\n",
    "####################RUN\n",
    "assigned_cells_df = pd.read_csv(assignedCell_path) \n",
    "\n",
    "comm_agg=AggreCellFeatures(text_folder,morph_folder,assigned_cells_df ,comm_agg_path)\n",
    "print('AGGREGATE CELL FEATURES DONE')\n",
    "final_df = concatCellSpat(comm_agg,spat,counts,id_df,final_df_path)\n",
    "print('MERGE CELL FEATURES WITH SPATIAL FEATURES DONE')\n",
    "\n",
    "plotROIdist(final_df,id_df)\n",
    "\n",
    "###########################################################################RUN#########################\n",
    "stage = 'exc44'\n",
    "id_df = pd.read_csv('/mnt/data10/shared/yujie/DCIS/patientNum_slideId_upstage_excision.csv')\n",
    "text_folder = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/{stage}_HE_cell_features/texture'\n",
    "morph_folder = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/{stage}_HE_cell_features/morphology'\n",
    "assignedCell_path = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/HE_cellAssignment/{stage}_assigned_LAMP2b_100_5.csv'\n",
    "\n",
    "#from process_spatialFunc.ipynb\n",
    "spat=pd.read_csv(f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/R_analysis/{stage}_L_100_5/spat_integration.csv')\n",
    "counts = pd.read_csv(f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/R_analysis/{stage}_L_100_5/ihc_counts.csv')\n",
    "\n",
    "\n",
    "result_dir = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/LAMP2b_100_5_result/{stage}'\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "comm_agg_path = os.path.join(result_dir,'commAgg_morph_text.csv')\n",
    "final_df_path = os.path.join(result_dir,'allFeatures.csv')\n",
    "json_path = os.path.join(result_dir,'split_index.json')\n",
    "\n",
    "#########################RUN\n",
    "assigned_cells_df = pd.read_csv(assignedCell_path) \n",
    "\n",
    "comm_agg=AggreCellFeatures(text_folder,morph_folder,assigned_cells_df ,comm_agg_path)\n",
    "print('AGGREGATE CELL FEATURES DONE')\n",
    "final_df = concatCellSpat(comm_agg,spat,counts,id_df,final_df_path)\n",
    "print('MERGE CELL FEATURES WITH SPATIAL FEATURES DONE')\n",
    "experiment_indexes = prepareSplit(final_df,id_df,json_path)\n",
    "print('GENERATE TRAIN/VALID SPLIT DONE')\n",
    "comm_agg = pd.read_csv(comm_agg_path)\n",
    "final_df = pd.read_csv(final_df_path)\n",
    "\n",
    "plotROIdist(final_df,id_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########simple cross-validation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import json\n",
    "import os\n",
    "def perform_fold_cv(final_df, id_df, json_path,n=5):\n",
    "    final_df = pd.merge(final_df, id_df[['slideId', 'Upstage']], on=\"slideId\")\n",
    "    \n",
    "    X = final_df.drop(columns=['Upstage'])\n",
    "    y = final_df['Upstage']\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_indexes = {}\n",
    "    fold_indexes[f'{n}_fold'] = {}\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        fold_indexes[f'{n}_fold'][fold] = {\n",
    "            'training_indexes': train_index.tolist(),\n",
    "            'validation_indexes': test_index.tolist()\n",
    "        }\n",
    "    \n",
    "    # Save to JSON\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(fold_indexes, f, indent=4)\n",
    "    \n",
    "    return fold_indexes\n",
    "\n",
    "# Assuming final_df and id_df are defined, and you have a valid json_path\n",
    "# fold_indexes = perform_5_fold_cv(final_df, id_df, 'path_to_your_file.json')\n",
    "\n",
    "stage = 'exc44'\n",
    "id_df = pd.read_csv('/mnt/data10/shared/yujie/DCIS/patientNum_slideId_upstage_excision.csv')\n",
    "result_dir = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/CL_100_5_result/{stage}'\n",
    "\n",
    "final_df_path = os.path.join(result_dir,'allFeatures.csv')\n",
    "json_path = os.path.join(result_dir,'5fold_split_index.json')\n",
    "\n",
    "final_df = pd.read_csv(final_df_path)\n",
    "\n",
    "fold_indexes = perform_fold_cv(final_df, id_df, json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def read_loss(result_dir,plot=True):\n",
    "    total_loss_dic = {}\n",
    "    for i in range(2,11):\n",
    "        total_loss_dic[f'latent{i}'] = {'mean':None,'std':None}\n",
    "        loss_list = []\n",
    "        for j in range(3,11):\n",
    "            loss_csv = os.path.join(result_dir,f'latent{i}_cluster{j}','min_losses.csv')\n",
    "            csv_df = pd.read_csv(loss_csv)\n",
    "            loss_list.append(pd.read_csv(loss_csv)['min_loss'].apply(lambda x:float(x[7:13])).mean())\n",
    "        total_loss_dic[f'latent{i}']['mean'] = np.mean(np.array(loss_list))\n",
    "        total_loss_dic[f'latent{i}']['std'] = np.std(np.array(loss_list))\n",
    "    dist_loss_dic = {}\n",
    "    for i in range(2,11):\n",
    "        dist_loss_dic[f'latent{i}'] = {'mean':None,'std':None}\n",
    "        loss_list = []\n",
    "        for j in range(3,11):\n",
    "            loss_csv = os.path.join(result_dir,f'latent{i}_cluster{j}','min_losses.csv')\n",
    "            csv_df = pd.read_csv(loss_csv)\n",
    "            loss_list.append(pd.read_csv(loss_csv)['min_dist_loss'].mean())\n",
    "        dist_loss_dic[f'latent{i}']['mean'] = np.mean(np.array(loss_list))\n",
    "        dist_loss_dic[f'latent{i}']['std'] = np.std(np.array(loss_list))\n",
    "    recons_loss_dic = {}\n",
    "    for i in range(2,11):\n",
    "        recons_loss_dic[f'latent{i}'] = {'mean':None,'std':None}\n",
    "        loss_list = []\n",
    "        for j in range(3,11):\n",
    "            loss_csv = os.path.join(result_dir,f'latent{i}_cluster{j}','min_losses.csv')\n",
    "            csv_df = pd.read_csv(loss_csv)\n",
    "            recons_loss = pd.read_csv(loss_csv)['min_loss'].apply(lambda x:float(x[7:13])).mean()-pd.read_csv(loss_csv)['min_dist_loss'].mean()\n",
    "            loss_list.append(recons_loss)\n",
    "        recons_loss_dic[f'latent{i}']['mean'] = np.mean(np.array(loss_list))\n",
    "        recons_loss_dic[f'latent{i}']['std'] = np.std(np.array(loss_list))\n",
    "\n",
    "\n",
    "    latent_i_values = range(2, 11)\n",
    "    mean_total = [total_loss_dic[f'latent{i}']['mean'] for i in latent_i_values]\n",
    "    mean_dist = [dist_loss_dic[f'latent{i}']['mean'] for i in latent_i_values]\n",
    "    mean_recons = [recons_loss_dic[f'latent{i}']['mean'] for i in latent_i_values]\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(latent_i_values, mean_total, marker='o',label='total')\n",
    "        plt.plot(latent_i_values, mean_dist, marker='o',label='kmeans')\n",
    "        plt.plot(latent_i_values, mean_recons, marker='o',label='reconstruction')\n",
    "\n",
    "        plt.xlabel('Latent i')\n",
    "        plt.ylabel('Mean Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    return np.mean(np.array(mean_total)) \n",
    "\n",
    "mean_total_list = []\n",
    "for beta in range(1,11):\n",
    "    result_dir = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/CL_100_5_result_new/exc44/beta0.{beta}'\n",
    "    if beta == 10:\n",
    "        result_dir = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/CL_100_5_result_new/exc44/beta1'\n",
    "    \n",
    "    mean_total_loss = read_loss(result_dir,True)\n",
    "    mean_total_list.append(mean_total_loss)\n",
    "\n",
    "beta_values = np.arange(0.1,1.1,0.1)\n",
    "plt.plot(beta_values, mean_total_list, marker='o',label='total')\n",
    "\n",
    "plt.xlabel('beta')\n",
    "plt.ylabel('Mean Total Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def read_loss_violin(result_dir, plot=True):\n",
    "    total_loss_dic = {}\n",
    "    min_kmeans_loss_clusters = {}\n",
    "    \n",
    "    for i in range(2, 11):\n",
    "        total_loss_dic[f'latent{i}'] = {'loss_list': None, 'mean': None, 'std': None}\n",
    "        loss_list = []\n",
    "        kmeans_loss_list = []\n",
    "        cluster_list = list(range(3, 11))\n",
    "        \n",
    "        for j in cluster_list:\n",
    "            loss_csv = os.path.join(result_dir, f'latent{i}_cluster{j}', 'min_losses.csv')\n",
    "            csv_df = pd.read_csv(loss_csv)\n",
    "            loss = pd.read_csv(loss_csv)['min_dist_loss'].mean()\n",
    "            loss_list.append(loss)\n",
    "            kmeans_loss = pd.read_csv(loss_csv)['min_dist_loss'].mean()\n",
    "            kmeans_loss_list.append(kmeans_loss)\n",
    "            \n",
    "        total_loss_dic[f'latent{i}']['loss_list'] = loss_list\n",
    "        total_loss_dic[f'latent{i}']['mean'] = np.mean(np.array(loss_list))\n",
    "        total_loss_dic[f'latent{i}']['std'] = np.std(np.array(loss_list))\n",
    "        min_kmeans_loss = min(kmeans_loss_list)\n",
    "        min_kmeans_loss_index = kmeans_loss_list.index(min_kmeans_loss)\n",
    "        min_kmeans_loss_clusters[f'latent{i}'] = cluster_list[min_kmeans_loss_index]\n",
    "\n",
    "    latent_i_values = range(2, 11)\n",
    "    mean_total = [total_loss_dic[f'latent{i}']['mean'] for i in latent_i_values]\n",
    "    \n",
    "    if plot:\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.violinplot(data=[loss_list['loss_list'] for _, loss_list in total_loss_dic.items()], ax=ax)\n",
    "        \n",
    "        for i, (latent, loss_list) in enumerate(total_loss_dic.items()):\n",
    "            min_loss = min(loss_list['loss_list'])\n",
    "            min_loss_index = loss_list['loss_list'].index(min_loss)\n",
    "            min_loss_cluster = min_kmeans_loss_clusters[latent]\n",
    "            ax.scatter([i], [min_loss], color='red', zorder=5)\n",
    "            ax.annotate(f'Cluster: {min_loss_cluster}', xy=(i, min_loss), xytext=(i + 0.2, min_loss - 0.02),\n",
    "                        arrowprops=dict(facecolor='black', arrowstyle='->'), fontsize=8)\n",
    "            \n",
    "        ax.set_xticklabels([f'{i}' for i in latent_i_values])\n",
    "        ax.set_xlabel('Latent i')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_title('Violin Plot of Kmeans jLoss for Different Latent i')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "    return np.mean(np.array(mean_total))\n",
    "\n",
    "mean_total_list = []\n",
    "for beta in range(1,11):\n",
    "    result_dir = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/CL_100_5_result_new/exc44/beta0.{beta}'\n",
    "    if beta == 10:\n",
    "        result_dir = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/CL_100_5_result_new/exc44/beta1'\n",
    "    \n",
    "    mean_total_loss = read_loss_violin(result_dir,True)\n",
    "    mean_total_list.append(mean_total_loss)\n",
    "\n",
    "beta_values = np.arange(0.1,1.1,0.1)\n",
    "plt.plot(beta_values, mean_total_list, marker='o',label='total')\n",
    "\n",
    "plt.xlabel('beta')\n",
    "plt.ylabel('Mean Total Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "for beta in range(1,11):\n",
    "    result_dir = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/CL_100_5_result_new/exc44/beta0.{beta}'\n",
    "    if beta == 10:\n",
    "        result_dir = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/CL_100_5_result_new/exc44/beta1'\n",
    "    dist_loss_dic = {}\n",
    "\n",
    "    i = 7\n",
    "    loss_list = []\n",
    "    for j in range(3,11):\n",
    "        loss_csv = os.path.join(result_dir,f'latent{i}_cluster{j}','min_losses.csv')\n",
    "        csv_df = pd.read_csv(loss_csv)\n",
    "        loss_list.append(pd.read_csv(loss_csv)['min_loss'].apply(lambda x:float(x[7:13])).mean())\\\n",
    "        \n",
    "    cluster_values = range(3,11)\n",
    "    plt.plot(cluster_values, loss_list, marker='o',label='total')\n",
    "\n",
    "    plt.xlabel('cluster')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "beta = 5\n",
    "result_dir = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/CL_100_5_result_new/exc44/train_all/beta0.{beta}'\n",
    "if beta == 10:\n",
    "    result_dir = f'/mnt/data10/shared/yujie/DCIS/ANALYSIS/CL_100_5_result_new/exc44/beta1'\n",
    "dist_loss_dic = {}\n",
    "\n",
    "i = 7\n",
    "total_loss_list = []\n",
    "dist_loss_list = []\n",
    "rec_loss_list=[]\n",
    "for j in range(3,11):\n",
    "    loss_csv = os.path.join(result_dir,f'latent{i}_cluster{j}','min_losses.csv')\n",
    "    csv_df = pd.read_csv(loss_csv)\n",
    "    total_loss_list.append(pd.read_csv(loss_csv)['min_loss'].values)\n",
    "    dist_loss_list.append(pd.read_csv(loss_csv)['min_dist_loss'].values)\n",
    "    rec_loss_list.append(pd.read_csv(loss_csv)['min_rec_loss'].values)\n",
    "cluster_values = range(3,11)\n",
    "\n",
    "plt.plot(cluster_values, total_loss_list, marker='o',label='total')\n",
    "plt.plot(cluster_values, dist_loss_list, marker='o',label='Kmeans')\n",
    "plt.plot(cluster_values, rec_loss_list, marker='o',label='reconstruction')\n",
    "plt.xlabel('cluster')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsmil",
   "language": "python",
   "name": "dsmil"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
